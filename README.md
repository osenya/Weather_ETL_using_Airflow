
Weather ETL Pipeline using Apache Airflow, Python, and DuckDB
This project demonstrates the design and implementation of a modular ETL (Extract, Transform, Load) pipeline using Apache Airflow, targeting real-time weather data from the OpenWeatherMap API. The pipeline is structured into three distinct stages â€” extract, transform, and load â€” and orchestrated through a directed acyclic graph (DAG) in Airflow.
ðŸ”§ Key Components:
â€¢	Extraction: A PythonOperator calls the OpenWeatherMap API to retrieve current weather data for Nairobi, then stores the raw JSON response using Airflow's XComs.
â€¢	Transformation: The pipeline parses and structures relevant weather metrics (timestamp, city, temperature, humidity, weather description) into a clean Pandas DataFrame, which is also passed between tasks via XComs.
â€¢	Loading: The final step loads the transformed data into a DuckDB table, ensuring persistent storage in a local database file. If the table doesn't exist, it's created automatically.
ðŸš€ Technologies Used:
â€¢	Airflow DAGs & PythonOperator for scheduling and orchestration
â€¢	Pandas for data transformation
â€¢	DuckDB for lightweight local data storage
â€¢	XComs for inter-task communication in Airflow
â€¢	OpenWeatherMap API as the live data source
This project exemplifies my ability to build a scalable, real-time data pipeline using modern tools and best practices in data engineering.

